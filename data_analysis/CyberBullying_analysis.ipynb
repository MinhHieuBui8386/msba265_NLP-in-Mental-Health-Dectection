{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries for data handling and text processing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv('/Users/roopasreesubramanyam/Desktop/msba265-finalstorage/data_storage/CyberBullying.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text-based data (Cyberbullying)  \\\n",
      "0                              No   \n",
      "1                               1   \n",
      "2                               2   \n",
      "3                               3   \n",
      "4                               4   \n",
      "\n",
      "                                          Unnamed: 1 Unnamed: 2    Unnamed: 3  \\\n",
      "0                                               Text     Emojis  Social Media   \n",
      "1                           u0 lmao wow fuck you too        ðŸ˜‚ ðŸ˜‚       YouTube   \n",
      "2  a white dress and red lipstick make everything...        NaN           NaN   \n",
      "3  this has been a trend since <number> of course...        NaN       YouTube   \n",
      "4  <user> <user> babies in cages destroying envir...        NaN       YouTube   \n",
      "\n",
      "       Unnamed: 4 Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  \\\n",
      "0            Type      Label         NaN         NaN         NaN         NaN   \n",
      "1  neutral/normal          0         NaN         NaN         NaN         NaN   \n",
      "2  neutral/normal          0         NaN         NaN         NaN         NaN   \n",
      "3  neutral/normal          0         NaN         NaN         NaN         NaN   \n",
      "4         neutral          0         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   ...  Unnamed: 16  Unnamed: 17  Unnamed: 18  Unnamed: 19  Unnamed: 20  \\\n",
      "0  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "1  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "2  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "3  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "4  ...          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  Unnamed: 25  \n",
      "0          NaN          NaN          NaN          NaN          NaN  \n",
      "1          NaN          NaN          NaN          NaN          NaN  \n",
      "2          NaN          NaN          NaN          NaN          NaN  \n",
      "3          NaN          NaN          NaN          NaN          NaN  \n",
      "4          NaN          NaN          NaN          NaN          NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Index(['Text-based data (Cyberbullying)', 'Unnamed: 1', 'Unnamed: 2',\n",
      "       'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7',\n",
      "       'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12',\n",
      "       'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16',\n",
      "       'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20',\n",
      "       'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24',\n",
      "       'Unnamed: 25'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows to get a sense of the data structure\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['No', 'Text', 'emoji', 'social media', 'type', 'label'] + [f'Unnamed: {i}' for i in range(6, len(df.columns))]\n",
    "# Rename columns to more readable names and handle any generic column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we don't need (e.g., 'emoji')\n",
    "df = df.drop(columns=['emoji'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only the columns we care about for this analysis\n",
    "df = df[['No', 'Text', 'social media', 'type', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].fillna('')\n",
    "\n",
    "# Replace any missing text values with empty strings so we don't run into errors during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# Load the set of common stop words from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text (split it into words) and make everything lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words (common words like 'the', 'is')\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    # Join the tokens back into a single string\n",
    "    return ' '.join(tokens)\n",
    "# Function to clean and preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the text preprocessing function to the 'Text' column\n",
    "df['processed_text'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0                                               Text   \n",
      "1                           u0 lmao wow fuck you too   \n",
      "2  a white dress and red lipstick make everything...   \n",
      "3  this has been a trend since <number> of course...   \n",
      "4  <user> <user> babies in cages destroying envir...   \n",
      "\n",
      "                                      processed_text  label  \n",
      "0                                               text  Label  \n",
      "1                                   u0 lmao wow fuck      0  \n",
      "2    white dress red lipstick make everything better      0  \n",
      "3  trend since number course wall street assumed ...      0  \n",
      "4  user user babies cages destroying environment ...      0  \n"
     ]
    }
   ],
   "source": [
    "# Check the processed data to make sure it looks right\n",
    "print(df[['Text', 'processed_text', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that 'label' is a number and drop any rows where it isn't\n",
    "df = df[pd.to_numeric(df['label'], errors='coerce').notnull()]\n",
    "df['label'] = df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the processed text into a numerical format using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = tfidf_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (8499, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF matrix shape:\", X.shape)\n",
    "\n",
    "# Display the shape of the TF-IDF matrix to check the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable 'y' as the 'label' column\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (5949, 5000) Test set size: (2550, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(\"Training set size:\", X_train.shape, \"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMOTE to create synthetic examples for underrepresented classes and RandomUnderSampler to balance overrepresented ones\n",
    "smote = SMOTE(random_state=42)\n",
    "under_sampler = RandomUnderSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that applies both oversampling and undersampling\n",
    "pipeline = Pipeline([('smote', smote), ('under_sampler', under_sampler)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data to create a balanced dataset\n",
    "X_train_res, y_train_res = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled training set size: (9426, 5000) Resampled labels size: (9426,)\n"
     ]
    }
   ],
   "source": [
    "# Check the new size of the balanced training data\n",
    "print(\"Resampled training set size:\", X_train_res.shape, \"Resampled labels size:\", y_train_res.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
